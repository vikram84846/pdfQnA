{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4ceb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4ddcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"uploads\",exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97cde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"uploads/genai-principles.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc43c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "async for page in pdf_loader.alazy_load():\n",
    "    pages.append(page)\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414c61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import  RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367cdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size= 1000)\n",
    "texts = text_spliter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e29ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d28194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veera\\OneDrive\\Desktop\\projects\\doc-reader\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "136b9a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f072c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f5f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "651276c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84278a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding,persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59791b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver =vectordb.as_retriever(search_type=\"similarity\",\n",
    "                      search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df598c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m retrvied_docs =\u001b[43mretriver\u001b[49m.invoke()\n",
      "\u001b[31mNameError\u001b[39m: name 'retriver' is not defined"
     ]
    }
   ],
   "source": [
    "# retrvied_docs =retriver.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915c7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(retrvied_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb976bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termed Beam Search, coined by Raj Reddy at CMU, oXen yields beYer results.\n",
      " Structuring iniMal text to elicit useful outputs from GenAI model is called prompt engineering.7\n",
      " See the full Krizhevshy, Sutskever, Hinton paper here.8\n",
      " See the Word2Vec paper here.9\n",
      " See the paper that introduced Transformers here.10\n",
      " See the GPT3 paper here.11\n",
      " See the instruct GPT paper here.12\n",
      " 12\n"
     ]
    }
   ],
   "source": [
    "# print(retrvied_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) predict the next word in a sequence based on preceding words, producing text by repeatedly sampling from a probability distribution.  LLMs generate varied and potentially meaningful completions due to the randomness of this sampling process.  This report focuses on LLMs due to their advanced development and the importance of language in information processing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import  ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temp=0.5,max_tokens= 500,google_api_key=\"your-api-key\" )\n",
    "from langchain.chains import  create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "system_prompt = (\n",
    "    \"\"\"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\"\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriver, question_answer_chain)\n",
    "response = rag_chain.invoke({\"input\": \"summarize this pdf\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
